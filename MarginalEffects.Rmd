---
title: "LAB 5"
author: "Juan Ramirez"
date: "2/19/2019"
output: html_document
---

##An Introduction to ‘margins’

`margins` is an effort to port Stata’s (closed source) margins command to R as an S3 generic method for calculating the marginal effects (or “partial effects”) of covariates included in model objects (like those of classes “lm” and “glm”). A plot method for the new “margins” class additionally ports the `marginsplot` command, and various additional functions support interpretation and visualization of such models.

With margins in R, replicating Stata’s results is incredibly simple using just the margins() method to obtain average marginal effects and its summary() method to obtain Stata-like output:

```{r echo=TRUE, message=FALSE}
library("margins")
x <- lm(mpg ~ cyl * hp + wt, data = mtcars)
(m <- margins(x))
summary(m)
```
With the exception of differences in rounding, the above results match identically what Stata’s margins command produces. Using the plot() method also yields an aesthetically similar result to Stata’s marginsplot:
```{r echo = TRUE, message=FALSE}
plot(m)
```

##Using Optional Arguments in `margins`
`margins` is intended as a port of (some of) the features of Stata’s margins command, which includes numerous options for calculating marginal effects at the mean values of a dataset (i.e., the marginal effects at the mean), an average of the marginal effects at each value of a dataset (i.e., the average marginal effect), marginal effects at representative values, and any of those operations on various subsets of a dataset. (The functionality of Stata’s command to produce predictive margins is not ported, as this is easily obtained from the prediction package.) In particular, Stata provides the following options:

- `at`: calculate marginal effects at (potentially representative) specified values (i.e., replacing observed values with specified replacement values before calculating marginal effects)
- `atmeans`: calculate marginal effects at the mean (MEMs) of a dataset rather than the default behavior of calculating average marginal effects (AMEs)
- `over`: calculate marginal effects (including MEMs and/or AMEs at observed or specified values) on subsets of the original data (e.g., the marginal effect of a treatment separately for men and women)

The `at` argument has been translated into `margins()` in a very similar manner. It can be used by specifying a list of variable names and specified values for those variables at which to calculate marginal effects, such as  margins(x, at = list(hp=150)). When using `at`, `margins()` constructs modified datasets - using `build_datalist()` - containing the specified values and calculates marginal effects on each modified dataset, `rbind`-ing them back into a single “margins” object.

Stata’s `atmeans` argument is not implemented in `margins()` for various reasons, including because it is possible to achieve the effect manually through an operation like `data$var <- mean(data$var, na.rm = TRUE)` and passing the modified data frame to `margins(x, data = data)`.

At present, `margins()` does not implement the `over` option. The reason for this is also simple: R already makes data subsetting operations quite simple using simple `[` extraction. If, for example, one wanted to calculate marginal effects on subsets of a data frame, those subsets can be passed directly to `margins()` via the data argument (as in a call to  `prediction()`).

##Using the `at` Argument
The `at` argument allows you to calculate marginal effects at representative cases (sometimes “MERs”) or marginal effects at means - or any other statistic - (sometimes “MEMs”), which are marginal effects for particularly interesting (sets of) observations in a dataset. This differs from marginal effects on subsets of the original data in that it operates on a modified set of the full dataset wherein particular variables have been replaced by specified values. This is helpful because it allows for calculation of marginal effects for counterfactual datasets (e.g., what if all women were instead men? what if all democracies were instead autocracies? what if all foreign cars were instead domestic?).

As an example, if we wanted to know if the marginal effect of horsepower (hp) on fuel economy differed across different types of automobile transmissions, we could simply use at to obtain separate marginal effect estimates for our data as if every car observation were a manual versus if every car observation were an automatic. The output of `margins()` is a simplified summary of the estimated marginal effects across the requested variable levels/combinations specified in at:
```{r echo =TRUE, message=FALSE}
x <- lm(mpg ~ cyl + wt + hp * am, data = mtcars)
margins(x, at = list(am = 0:1))
```
Because of the hp-am interaction in the regression, the marginal effect of horsepower differs between the two sets of results. We can also specify more than one variable to `at`, creating a potentially long list of marginal effects results. For example, we can produce marginal effects at both levels of `am` and the values from the five-number summary (minimum, Q1, median, Q3, and maximum) of observed values of `hp`. This produces 2 * 5 = 10 sets of marginal effects estimates:

```{r echo=TRUE, message=FALSE}
margins(x, at = list(am = 0:1, hp = fivenum(mtcars$hp)))
```

Because this is a linear model, the marginal effects of `cyl` and `wt` do not vary across levels of `am` or `hp`. The minimum and Q1 value of `hp` are also the same, so the marginal effects of `am` are the same in the first two results. As you can see, however, the marginal effect of `hp` differs when `am == 0` versus `am == 1` (first and second rows) and the marginal effect of `am` differs across levels of `hp` (e.g., between the first and third rows). As should be clear, the `at` argument is incredibly useful for getting a better grasp of the marginal effects of different covariates.

This becomes especially apparent when a model includes power-terms (or any other alternative functional form of a covariate). Consider, for example, the simple model of fuel economy as a function of weight, with weight included as both a first- and second-order term:

```{r echo=TRUE, message=FALSE}
x <- lm(mpg ~ wt + I(wt^2), data = mtcars)
summary(x)
```

Looking only at the regression results table, it is actually quite difficult to understand the effect of `wt` on fuel economy because it requires performing mental multiplication and addition on all possible values of `wt`. Using the `at` option to `margins`, you could quickly obtain a sense of the average marginal effect of `wt` at a range of plausible values:

```{r echo=TRUE, message=FALSE}
margins(x, at = list(wt = fivenum(mtcars$wt)))
```

The marginal effects in the first column of results reveal that the average marginal effect of `wt` is large and negative except when `wt` is very large, in which case it has an effect not distinguishable from zero. We can easily plot these results using the `cplot()` function to see the effect visually in terms of either predicted fuel economy or the marginal effect of `wt`:

```{r echo=TRUE, message=FALSE}
cplot(x, "wt", what = "prediction", main = "Predicted Fuel Economy, Given Weight")
cplot(x, "wt", what = "effect", main = "Average Marginal Effect of Weight")
```

A really nice feature of Stata’s `margins` command is that it handles factor variables gracefully. This functionality is difficult to emulate in R, but the `margins()` function does its best. Here we see the marginal effects of a simple regression that includes a factor variable:

```{r echo=TRUE, message=FALSE}
x <- lm(mpg ~ factor(cyl) * hp + wt, data = mtcars)
margins(x)
```

`margins()` recognizes the factor and displays the marginal effect for each level of the factor separately. (Caveat: this may not work with certain `at` specifications, yet.)


##Interpreting Interactions with Marginal Effects
One of principal motives for developing `margins` is to facilitate the substantive interpretation of interaction terms in regression models. A large literature now describes the difficulties of such interpretations in both linear and non-linear regression models. This vignette walks through some of that interpretation.

###Interactions in OLS
If we begin with a simple example of a regression model with an interaction term, the difficulties of interpretation become almost immediately clear. In this first model, we’ll use the `mtcars` dataset to understand vehicle fuel economy as a function of `drat` (rear axle ratio), `wt` (weight), and their interaction. As Brambor et al. (2006) make clear, the most common mistake in such models is estimating the model without the constituent variables. We can see why this is a problem by estimating the model with and without the constituent terms:

```{r echo=TRUE, message=FALSE}
summary(lm(mpg ~ drat:wt, data = mtcars))
summary(lm(mpg ~ drat * wt, data = mtcars))
```

Clearly the models produce radically different estimates and goodness-of-fit to the original data. As a result, it’s important to use all constituent terms in the model even if they are thought a priori to have coefficients of zero. Now let’s see how we can use  `margins()` to interpret a more complex three-way interaction:

```{r echo=TRUE, message=FALSE}
x1 <- lm(mpg ~ drat * wt * am, data = mtcars)
summary(margins(x1))
```

By default, `margins()` will supply the average marginal effects of the constituent variables in the model. Note how the `drat:wt` term is not expressed in the margins results. This is because the contribution of the `drat:wt` term is incorporated into the marginal effects for the constituent variables. Because there is a significant interaction, we can see this by examining margins at different levels of the constituent variables. The `drat` variable is continuous, taking values from 2.76 to 4.93:

```{r echo=TRUE, message=FALSE}
margins(x1, at = list(drat = range(mtcars$drat)))
```

Now `margins()` returns two "margins" objects, one for each combination of values specified in drat. We can see in the above that cars with a low axle ratio (drat == 2.76), the average marginal effect of weight is a reduction in fuel economy of 4.99 miles per gallon. For vehicles with a higher ratio (drat == 4.93), this reduction in fuel economy is lower at 7.27 miles per gallon. Yet this is also not fully accurate because it mixes the automatic and manual cars, so we may want to further break out the results by transmission type.

The `at` argument accepts multiple named combinations of variables, so we can specify particular values of both drat and wt and am for which we would like to understand the marginal effect of each variable. For example, we might want to look at the effects of each variable for vehicles with varying axle ratios but also across some representative values of the `wt` distribution, separately for manual and automatic vehicles. (Note that the order of values in the at object does matter and it can inclue variables that are not explicitly being modelled).

```{r echo=TRUE, message=FALSE}
wts <- prediction::seq_range(mtcars$wt, 10)
m1 <- margins(x1, at = list(wt = wts, drat = range(mtcars$drat), am = 0:1))
nrow(m1)/nrow(mtcars)
```

As you can see above, the result is a relatively long data frame (basically a stack of "margins" objects specific to each of the - 40 - requested combinations of `at` values). We can examine the whole stack using `summary()`, but it’s an extremely long output (one row for each estimate times the number of unique combinations of at values, so 120 rows in this instance).

An easier way to understand all of this is to use graphing. The `cplot()` function, in particular, is useful here. We can, for example, plot the marginal effect of axle ratio across levels of vehicle weight, separately for automatic vehicles (in red) and manual vehicles (in blue):

```{r echo=TRUE, message=FALSE}
cplot(x1, x = "wt", dx = "drat", what = "effect", 
      data = mtcars[mtcars[["am"]] == 0,], 
      col = "red", se.type = "shade",
      xlim = range(mtcars[["wt"]]), ylim = c(-20, 20), 
      main = "AME of Axle Ratio on Fuel Economy")
cplot(x1, x = "wt", dx = "drat", what = "effect", 
      data = mtcars[mtcars[["am"]] == 1,], 
      col = "blue", se.type = "shade", 
      draw = "add")
```

`cplot()` only provides AME displays over the observed range of the data (noted by the rug), so we can see that it would be inappropriate to compare the average marginal effect of axle ratio for the two transmission types at extreme values of weight.

Another use of these types of plots can be to interpret power terms, to see the average marginal effect of a variable across values of itself. Consider the following, for example:

```{r echo=TRUE, message=FALSE}
x1b <- lm(mpg ~ am * wt + am * I(wt^2), data = mtcars)
cplot(x1b, x = "wt", dx = "wt", what = "effect", 
      data = mtcars[mtcars[["am"]] == 0,], 
      col = "red", se.type = "shade",
      xlim = range(mtcars[["wt"]]), ylim = c(-20, 20), 
      main = "AME of Weight on Fuel Economy")
cplot(x1b, x = "wt", dx = "wt", what = "effect", 
      data = mtcars[mtcars[["am"]] == 1,], 
      col = "blue", se.type = "shade", 
      draw = "add")
```

Note, however, that interpreting these kind of continuous-by-continuous interaction terms is slightly more complex because the marginal effect of both constituent variables always depends on the level of the other variable. We’ll use the horsepower (hp) variable from mtcars to understand this type of interaction. We can start by looking at the AMEs:

```{r echo=TRUE, message=FALSE}
x2 <- lm(mpg ~ hp * wt, data = mtcars)
margins(x2)
```

On average across the cases in the dataset, the effect of horsepower is slightly negative. On average, the effect of weight is also negative. Both decrease fuel economy. But what is the marginal effect of each variable across the range of values we actually observe in the data. To get a handle on this, we can use the persp() method provided by margins.

```{r echo=TRUE, message=FALSE}
persp(x2, "wt", "hp", theta = c(45, 135, 225, 315), what = "effect")
```

To make sense of this set of plots (actually, the same plot seen from four different angles), it will also be helpful to have the original regression results close at-hand:

```{r echo=TRUE, message=FALSE}
summary(x2)
```

If we express the regression results as an equation: `mpg = 49.81 + (-0.12 * hp) + (-8.22 * wt) + (0.03 * hp*wt)`, it will be easy to see how the three-dimensional surface above reflects various partial derivatives of the regression equation.

For example, if we take the partial derivative of the regression equation with respect to `wt` (i.e., the marginal effect of weight), the equation is: `d_mpg/d_wt = (-8.22) + (0.03 * hp)`. This means that the marginal effect of weight is large and negative when horsepower is zero (which never occurs in the mtcars dataset) and decreases in magnitude and becoming more positive as horsepower increases. We can see this in the above graph that the marginal effect of weight is constant across levels of weight because wt does not enter into the partial derivative. Across levels, of horsepower, however, the marginal effect becomes more positive. This is clear looking at the “front” or “back” edges of the surface, which are straight-linear increases. The slope of those edges is 0.03 (the coefficient on the interaction term).

If we then take the partial derivative with respect to hp (to obtain the marginal effect of horsepower), the equation is:  `d_mpg/d_hp = (-0.12) + (0.03 * wt)`. When `wt` is zero, this partial derivative (or marginal effect) is -0.12 miles/gallon. The observed range of `wt`, however, is only: 1.513, 5.424. We can see these results in the analogous graph of the marginal effects of horsepower (below). The “front” and “back” edges of the graph are now flat (reflecting how the marginal effect of horsepower is constant across levels of horsepower), while the “front-left” and “right-back” edges of the surface are lines with slope 0.03, reflecting the coefficient on the interaction term.

```{r echo=TRUE, message=FALSE}
persp(x2, "hp", "wt", theta = c(45, 135, 225, 315), what = "effect")
```

An alternative way of plotting these results is to take “slices” of the three-dimensional surface and present them in a two-dimensional graph. That strategy would be especially appropriate for a categorical-by-continuous interaction where the categories of the first variable did not necessarily have a logical ordering sufficient to draw a three-dimensional surface.


##ggplot2 examples
It should be noted that `cplot()` returns a fairly tidy data frmae, making it possible to use ggplot2 as an alternative plotting package to display the kinds of relationships of typical interest. For example, returning to our earlier example:

```{r echo=TRUE, message=FALSE}
x1 <- lm(mpg ~ drat * wt * am, data = mtcars)
cdat <- cplot(x1, "wt", draw = FALSE)
head(cdat)
```

From this structure, it is very easy to draw a predicted values plot

```{r echo=TRUE, message=FALSE}
library("ggplot2")
ggplot(cdat, aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0) +
  ggtitle("Predicted Fuel Economy (mpg) by Weight") +
  xlab("Weight (1000 lbs)") + ylab("Predicted Value")
```

And the same thing is possible with a marginal effect calculation:

```{r echo=TRUE, message=FALSE}
cdat <- cplot(x1, "wt", "drat", what = "effect", draw = FALSE)
ggplot(cdat, aes(x = xvals)) + 
  geom_line(aes(y = yvals)) +
  geom_line(aes(y = upper), linetype = 2) +
  geom_line(aes(y = lower), linetype = 2) +
  geom_hline(yintercept = 0) +
  ggtitle("AME of Axle Ratio on Fuel Economy (mpg) by Weight") +
  xlab("Weight (1000 lbs)") + ylab("AME of Axle Ratio")
```
















